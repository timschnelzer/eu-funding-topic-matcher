{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8424d072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# 03 Semantic Matching with Multi-Modal Embeddings\n",
    "# ===================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40047473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 0) Install dependencies ---\n",
    "# pip install sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a18fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Imports ---\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import faiss\n",
    "import ast\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934ac326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2) Configuration ---\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
    "PROC_DIR = os.path.join(DATA_DIR, \"processed\")\n",
    "os.makedirs(PROC_DIR, exist_ok=True)\n",
    "\n",
    "CSV_PATH = os.path.join(PROC_DIR, \"cleaned_project_data.csv\")\n",
    "DOWNLOAD_URL = \"https://drive.google.com/uc?id=1A1j9JsYjmD1EuanF9FFWsgi2wGbKQ1bg&export=download\"\n",
    "\n",
    "# Column names\n",
    "PROJECT_TEXT_COL = \"project_text_simple\"\n",
    "TOPIC_ID_COL = \"topics\"\n",
    "TOPIC_TEXT_COL = \"topic_text\"\n",
    "PROJECT_KEYWORDS_COL = \"keywords_clean\"\n",
    "\n",
    "# Model settings\n",
    "\n",
    "# NOTE: Originally, 'multi-qa-mpnet-base-dot-v1' was tested and gave similar evaluation results.\n",
    "# However, in practice, when using longer project texts along with multi-modal features\n",
    "# (keywords + year vectors), it caused errors or failed to handle longer input efficiently.\n",
    "# Therefore, 'all-MiniLM-L6-v2' was chosen for robust, scalable processing in this pipeline.\n",
    "\n",
    "EMBED_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "EVAL_TOP_K = 10\n",
    "BATCH_ENCODE = 64\n",
    "\n",
    "# Optional: small batch for cross-encoder demo\n",
    "# Important Note: Cross-encoder might appear worse on small batches, but it is much slower and not feasible for full dataset retrieval.\n",
    "\n",
    "CROSS_ENCODER_BATCH = 50 # Adjust based on your hardware capabilities\n",
    "CROSS_ENCODER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85f5dfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: 4429 projects\n",
      "Unique topics: 1873\n"
     ]
    }
   ],
   "source": [
    "# --- 3) Load Data (download if missing) ---\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    print(f\"{CSV_PATH} not found locally. Downloading from Drive...\")\n",
    "    response = requests.get(DOWNLOAD_URL)\n",
    "    if response.status_code == 200:\n",
    "        with open(CSV_PATH, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded dataset to {CSV_PATH}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Failed to download dataset. HTTP Status: {response.status_code}\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, encoding=\"utf-8\")\n",
    "print(f\"Loaded dataset: {len(df)} projects\")\n",
    "\n",
    "# Safely parse keywords\n",
    "df[PROJECT_KEYWORDS_COL] = df[PROJECT_KEYWORDS_COL].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "# Extract unique topics\n",
    "df_topics = df.groupby(TOPIC_ID_COL).first().reset_index()\n",
    "print(f\"Unique topics: {len(df_topics)}\")\n",
    "\n",
    "# Identify year columns\n",
    "year_cols = [col for col in df.columns if col.startswith('year_')]\n",
    "required_cols = [PROJECT_TEXT_COL, TOPIC_ID_COL, TOPIC_TEXT_COL, PROJECT_KEYWORDS_COL] + year_cols\n",
    "assert all(col in df.columns for col in required_cols), \"Missing columns in DataFrame\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42d4f7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 'all-MiniLM-L6-v2', embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# --- 4) Load Embedding Model ---\n",
    "model = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "embedding_dim = model.get_sentence_embedding_dimension()\n",
    "print(f\"Loaded '{EMBED_MODEL_NAME}', embedding dimension: {embedding_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6628b345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5) Helper: Encode & Cache ---\n",
    "def encode_and_cache(texts, path, model, batch_size=BATCH_ENCODE):\n",
    "    if os.path.exists(path):\n",
    "        print(f\"Loading embeddings from {path}\")\n",
    "        return np.load(path)\n",
    "    else:\n",
    "        print(f\"Creating embeddings and saving to {path}\")\n",
    "        embeddings = model.encode(texts, show_progress_bar=True, batch_size=batch_size)\n",
    "        np.save(path, embeddings)\n",
    "        return embeddings\n",
    "    \n",
    "# Paths for the .npy files\n",
    "project_embeddings_path = os.path.join(PROC_DIR, 'project_embeddings.npy')\n",
    "topic_embeddings_path   = os.path.join(PROC_DIR, 'topic_text_embeddings.npy')\n",
    "project_keyword_embeddings_path = os.path.join(PROC_DIR, 'project_keyword_embeddings.npy')\n",
    "topic_keyword_embeddings_path   = os.path.join(PROC_DIR, 'topic_keyword_embeddings.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2742f45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings and saving to /Users/timschnelzer/Developer/tender-matching-horizon-europe/data/processed/project_embeddings.npy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cafc1310c2ed4be6a56bb543887d502a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings and saving to /Users/timschnelzer/Developer/tender-matching-horizon-europe/data/processed/topic_text_embeddings.npy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d4cb0c36284f0c9458485b098b0d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings and saving to /Users/timschnelzer/Developer/tender-matching-horizon-europe/data/processed/project_keyword_embeddings.npy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237f486dce6747b88b5750e8609e1b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings and saving to /Users/timschnelzer/Developer/tender-matching-horizon-europe/data/processed/topic_keyword_embeddings.npy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67cf4450cb3149bb9db585015918dd41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All individual embeddings created and saved for application use.\n"
     ]
    }
   ],
   "source": [
    "# --- 6) Create Embeddings (Unified and Cached for App Use) ---\n",
    "\n",
    "# Paths for cached embeddings\n",
    "project_embeddings_path = os.path.join(PROC_DIR, 'project_embeddings.npy')\n",
    "topic_embeddings_path = os.path.join(PROC_DIR, 'topic_text_embeddings.npy')\n",
    "project_keyword_embeddings_path = os.path.join(PROC_DIR, 'project_keyword_embeddings.npy')\n",
    "topic_keyword_embeddings_path = os.path.join(PROC_DIR, 'topic_keyword_embeddings.npy')\n",
    "\n",
    "# Helper function: Encode & cache\n",
    "def encode_and_cache(texts, path, model, batch_size=BATCH_ENCODE):\n",
    "    if os.path.exists(path):\n",
    "        print(f\"Loading embeddings from {path}\")\n",
    "        return np.load(path)\n",
    "    else:\n",
    "        print(f\"Creating embeddings and saving to {path}\")\n",
    "        embeddings = model.encode(texts, show_progress_bar=True, batch_size=batch_size)\n",
    "        np.save(path, embeddings)\n",
    "        return embeddings\n",
    "\n",
    "# 1) Project text embeddings\n",
    "project_text_embeddings = encode_and_cache(df[PROJECT_TEXT_COL].tolist(), project_embeddings_path, model)\n",
    "\n",
    "# 2) Topic text embeddings\n",
    "topic_text_embeddings = encode_and_cache(df_topics[TOPIC_TEXT_COL].tolist(), topic_embeddings_path, model)\n",
    "\n",
    "# 3) Project keyword embeddings\n",
    "project_keyword_embeddings = encode_and_cache(\n",
    "    df[PROJECT_KEYWORDS_COL].apply(lambda x: ' '.join(x)).tolist(),\n",
    "    project_keyword_embeddings_path,\n",
    "    model\n",
    ")\n",
    "\n",
    "# 4) Topic keyword embeddings (aggregated first)\n",
    "topic_keywords_agg = df.groupby(TOPIC_ID_COL)[PROJECT_KEYWORDS_COL].apply(\n",
    "    lambda lists: list(set([kw for sublist in lists for kw in sublist]))\n",
    ").reset_index()\n",
    "topic_keywords_agg = pd.merge(df_topics[[TOPIC_ID_COL]], topic_keywords_agg, on=TOPIC_ID_COL, how='left')\n",
    "topic_keywords_agg[PROJECT_KEYWORDS_COL] = topic_keywords_agg[PROJECT_KEYWORDS_COL].fillna('').apply(lambda x: ' '.join(x))\n",
    "\n",
    "topic_keyword_embeddings = encode_and_cache(\n",
    "    topic_keywords_agg[PROJECT_KEYWORDS_COL].tolist(),\n",
    "    topic_keyword_embeddings_path,\n",
    "    model\n",
    ")\n",
    "\n",
    "print(\"All individual embeddings created and saved for application use.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca0066c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created. Entries: 1873\n",
      "\n",
      "--- FAISS Bi-Encoder Evaluation ---\n",
      "Top-10 Accuracy: 0.9564\n",
      "Top-1 Accuracy: 0.8905\n",
      "MRR@10: 0.9148\n"
     ]
    }
   ],
   "source": [
    "# --- 7) Build FAISS Index & Evaluate ---\n",
    "# Combine topic embeddings on the fly for FAISS\n",
    "topic_embeddings_combined = np.hstack([topic_text_embeddings, topic_keyword_embeddings, df_topics[year_cols].values]).astype('float32')\n",
    "faiss.normalize_L2(topic_embeddings_combined)\n",
    "\n",
    "index = faiss.IndexFlatIP(topic_embeddings_combined.shape[1])\n",
    "index.add(topic_embeddings_combined)\n",
    "print(f\"FAISS index created. Entries: {index.ntotal}\")\n",
    "\n",
    "# Evaluate Top-K retrieval on the full dataset\n",
    "top_k_hits = 0\n",
    "top_1_hits = 0\n",
    "mrr_sum = 0.0\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    proj_vec = np.hstack([\n",
    "        project_text_embeddings[i:i+1],\n",
    "        project_keyword_embeddings[i:i+1],\n",
    "        df[year_cols].values[i:i+1]\n",
    "    ]).astype('float32')\n",
    "    faiss.normalize_L2(proj_vec)\n",
    "\n",
    "    true_topic = row[TOPIC_ID_COL]\n",
    "    true_idx_series = df_topics[df_topics[TOPIC_ID_COL]==true_topic].index\n",
    "    if true_idx_series.empty: continue\n",
    "    true_idx = true_idx_series[0]\n",
    "\n",
    "    D, I = index.search(proj_vec, EVAL_TOP_K)\n",
    "    ranked_idx = I[0]\n",
    "\n",
    "    if ranked_idx[0] == true_idx: top_1_hits += 1\n",
    "    if true_idx in ranked_idx:\n",
    "        top_k_hits += 1\n",
    "        rank = np.where(ranked_idx == true_idx)[0][0] + 1\n",
    "        mrr_sum += 1.0 / rank\n",
    "\n",
    "top_k_acc = top_k_hits / len(df)\n",
    "top_1_acc = top_1_hits / len(df)\n",
    "mrr_at_k = mrr_sum / len(df)\n",
    "\n",
    "print(\"\\n--- FAISS Bi-Encoder Evaluation ---\")\n",
    "print(f\"Top-{EVAL_TOP_K} Accuracy: {top_k_acc:.4f}\")\n",
    "print(f\"Top-1 Accuracy: {top_1_acc:.4f}\")\n",
    "print(f\"MRR@{EVAL_TOP_K}: {mrr_at_k:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbc6448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8) Cross-Encoder Demo ---\n",
    "cross_demo_df = df.head(CROSS_ENCODER_BATCH)\n",
    "cross_model = CrossEncoder(CROSS_ENCODER_MODEL)\n",
    "\n",
    "top_k_hits_ce = 0\n",
    "top_1_hits_ce = 0\n",
    "mrr_ce = 0.0\n",
    "\n",
    "for idx, row in cross_demo_df.iterrows():\n",
    "    project_text = row[PROJECT_TEXT_COL]\n",
    "    true_topic = row[TOPIC_ID_COL]\n",
    "\n",
    "    pairs = [[project_text, t_text] for t_text in df_topics[TOPIC_TEXT_COL]]\n",
    "    scores = cross_model.predict(pairs)\n",
    "    ranked_idx = scores.argsort()[::-1][:EVAL_TOP_K]\n",
    "    top_topics = df_topics.iloc[ranked_idx][TOPIC_ID_COL].tolist()\n",
    "\n",
    "    if top_topics[0] == true_topic: top_1_hits_ce += 1\n",
    "    if true_topic in top_topics:\n",
    "        top_k_hits_ce += 1\n",
    "        rank = top_topics.index(true_topic) + 1\n",
    "        mrr_ce += 1.0 / rank\n",
    "\n",
    "top_k_acc_ce = top_k_hits_ce / len(cross_demo_df)\n",
    "top_1_acc_ce = top_1_hits_ce / len(cross_demo_df)\n",
    "mrr_ce /= len(cross_demo_df)\n",
    "\n",
    "print(\"\\n--- Cross-Encoder (Sample) Evaluation ---\")\n",
    "print(f\"Top-{EVAL_TOP_K} Accuracy: {top_k_acc_ce:.4f}\")\n",
    "print(f\"Top-1 Accuracy: {top_1_acc_ce:.4f}\")\n",
    "print(f\"MRR@{EVAL_TOP_K}: {mrr_ce:.4f}\")\n",
    "\n",
    "print(\"\\nNote: Cross-Encoder was applied to a small batch only for demonstration due to computational cost.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
